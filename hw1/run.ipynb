{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import importlib\n",
    "\n",
    "import data_utils\n",
    "import models\n",
    "import train_utils\n",
    "import evaluation\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(models)\n",
    "importlib.reload(train_utils)\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data and put in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(np.load(\"./data/vocab.npy\"))\n",
    "data_path = \"data\"\n",
    "train_file = \"train_real.csv\"\n",
    "val_file = \"val.csv\"\n",
    "test_file = \"holdout_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = data_utils.prep_train_val(\n",
    "    data_path, train_file, val_file, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.MultiLayerMLP(emb_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerMLP(\n",
       "  (embedding): Embedding(2043, 300)\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=300, out_features=46, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-0962039a045a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m best_model, val_loss, val_f1 = train_utils.train(\n\u001b[0;32m----> 2\u001b[0;31m     train_data, val_data, m)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/CSE244_ML_for_NLP/hw1/train_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, val_data, model, lr, patience, max_epoch, print_freq)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mno_improvement\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             print('Epoch: {}, LR: {}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val F1 score {:.4f}'.format(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, metrics, epoch)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;31m# convert `metrics` to float, in case it's a zero-dim Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "best_model, val_loss, val_f1 = train_utils.train(\n",
    "    train_data, val_data, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter searching - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for num_layer in range(1, 6):\n",
    "    print(\"number of layers\", num_layer)\n",
    "    model = MultiLayerMLP(num_layers=num_layer)\n",
    "    best_model, best_val_loss, best_val_f1 = train(train_data, val_data, model)\n",
    "    models.append({\"num_layer\": num_layer,\n",
    "                   \"trained_model\": best_model, \n",
    "                   \"best_val_f1\": best_val_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for emb_dim in [10, 20, 40, 80, 100, 150, 200, 250, 300, 400, 500]:\n",
    "    model = MultiLayerMLP(num_layers=2, emb_dim=emb_dim)\n",
    "    best_model, best_val_loss, best_val_f1 = train(train_data, val_data, model)\n",
    "    models.append({\"emb_dim\": emb_dim,\n",
    "                   \"trained_model\": best_model, \n",
    "                   \"best_val_f1\": best_val_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for p in [0, 0.1, 0.2, 0.3, 0.4]:\n",
    "    print(\"p\", p)\n",
    "    model = MultiLayerMLP(num_layers=2, emb_dim=500, p_dropout=p)\n",
    "    best_model, best_val_loss, best_val_f1 = train(train_data, val_data, model)\n",
    "    models.append({\"dropout\": p, \n",
    "                   \"trained_model\": best_model, \n",
    "                   \"best_val_f1\": best_val_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Searching Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dims = [100, 200, 300, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for emb_dim in emb_dims:\n",
    "    for dropout in [0, 0.1, 0.2, 0.3, 0.4]:\n",
    "        print(emb_dim, dropout, bi)\n",
    "        model = BILSTM(emb_dim=emb_dim, num_layers=2, dropout=dropout, bi=True)\n",
    "        best_model, best_val_loss, best_val_f1, = train(train_data, val_data, model, lr=1e-2)\n",
    "        models.append({\"emb_dim\": emb_dim,\n",
    "                       \"dropout\": dropout, \n",
    "                       \"trained_model\": best_model, \n",
    "                       \"best_val_f1\": best_val_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, LR: 0.01, Train Loss: 0.0530, Val Loss: 0.0145, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.01, Train Loss: 0.0089, Val Loss: 0.0116, Val F1 score 0.4486\n",
      "Epoch: 20, LR: 0.0001, Train Loss: 0.0069, Val Loss: 0.0114, Val F1 score 0.4714\n",
      "40\n",
      "Epoch: 0, LR: 0.01, Train Loss: 0.0402, Val Loss: 0.0134, Val F1 score 0.1131\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0058, Val Loss: 0.0102, Val F1 score 0.4989\n",
      "Epoch: 20, LR: 0.0001, Train Loss: 0.0051, Val Loss: 0.0100, Val F1 score 0.5020\n",
      "80\n",
      "Epoch: 0, LR: 0.01, Train Loss: 0.0325, Val Loss: 0.0123, Val F1 score 0.2172\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0040, Val Loss: 0.0111, Val F1 score 0.5272\n",
      "Epoch: 20, LR: 0.0001, Train Loss: 0.0031, Val Loss: 0.0110, Val F1 score 0.5378\n",
      "100\n",
      "Epoch: 0, LR: 0.01, Train Loss: 0.0322, Val Loss: 0.0115, Val F1 score 0.2392\n",
      "Epoch: 10, LR: 0.01, Train Loss: 0.0023, Val Loss: 0.0089, Val F1 score 0.6197\n",
      "Epoch: 20, LR: 0.001, Train Loss: 0.0010, Val Loss: 0.0089, Val F1 score 0.6418\n",
      "150\n",
      "Epoch: 0, LR: 0.01, Train Loss: 0.0305, Val Loss: 0.0124, Val F1 score 0.2883\n",
      "Epoch: 10, LR: 0.01, Train Loss: 0.0030, Val Loss: 0.0095, Val F1 score 0.5872\n",
      "Epoch: 20, LR: 0.0001, Train Loss: 0.0014, Val Loss: 0.0096, Val F1 score 0.6178\n",
      "Epoch: 30, LR: 1e-05, Train Loss: 0.0013, Val Loss: 0.0096, Val F1 score 0.6258\n",
      "200\n",
      "Epoch: 0, LR: 0.01, Train Loss: 0.0288, Val Loss: 0.0117, Val F1 score 0.2701\n",
      "Epoch: 10, LR: 0.01, Train Loss: 0.0025, Val Loss: 0.0097, Val F1 score 0.6297\n",
      "Epoch: 20, LR: 0.0001, Train Loss: 0.0014, Val Loss: 0.0093, Val F1 score 0.6498\n",
      "300\n",
      "Epoch: 0, LR: 0.01, Train Loss: 0.0275, Val Loss: 0.0120, Val F1 score 0.2873\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0062, Val Loss: 0.0126, Val F1 score 0.4472\n",
      "Epoch: 20, LR: 1e-05, Train Loss: 0.0059, Val Loss: 0.0127, Val F1 score 0.4512\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for unit in [20, 40, 80, 100, 150, 200, 300]:\n",
    "    print(unit)\n",
    "    model = BILSTM(emb_dim=500, lstm_unit=unit)\n",
    "    best_model, best_val_loss, best_val_f1 = train(train_data, val_data, model, lr=1e-2)\n",
    "    models.append({\"lstm_unit\": unit,\n",
    "                   \"trained_model\": best_model, \n",
    "                   \"best_val_f1\": best_val_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_val_f1</th>\n",
       "      <th>lstm_unit</th>\n",
       "      <th>trained_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.653468</td>\n",
       "      <td>20</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.653468</td>\n",
       "      <td>40</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.653468</td>\n",
       "      <td>80</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.653468</td>\n",
       "      <td>100</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.653468</td>\n",
       "      <td>150</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.653468</td>\n",
       "      <td>200</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.653468</td>\n",
       "      <td>300</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_val_f1  lstm_unit                                      trained_model\n",
       "0     0.653468         20  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...\n",
       "1     0.653468         40  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...\n",
       "2     0.653468         80  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...\n",
       "3     0.653468        100  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...\n",
       "4     0.653468        150  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...\n",
       "5     0.653468        200  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...\n",
       "6     0.653468        300  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n..."
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, LR: 0.001, Train Loss: 0.0749, Val Loss: 0.0157, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0114, Val Loss: 0.0101, Val F1 score 0.3485\n",
      "Epoch: 20, LR: 0.0001, Train Loss: 0.0080, Val Loss: 0.0103, Val F1 score 0.4146\n",
      "Epoch: 30, LR: 1e-05, Train Loss: 0.0076, Val Loss: 0.0102, Val F1 score 0.4106\n",
      "100\n",
      "2\n",
      "Epoch: 0, LR: 0.001, Train Loss: 0.0649, Val Loss: 0.0150, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0116, Val Loss: 0.0092, Val F1 score 0.4064\n",
      "Epoch: 20, LR: 0.001, Train Loss: 0.0037, Val Loss: 0.0063, Val F1 score 0.6616\n",
      "Epoch: 30, LR: 0.001, Train Loss: 0.0015, Val Loss: 0.0060, Val F1 score 0.7089\n",
      "Epoch: 40, LR: 1e-05, Train Loss: 0.0011, Val Loss: 0.0057, Val F1 score 0.7406\n",
      "Epoch: 50, LR: 1.0000000000000002e-07, Train Loss: 0.0010, Val Loss: 0.0057, Val F1 score 0.7306\n",
      "100\n",
      "3\n",
      "Epoch: 0, LR: 0.001, Train Loss: 0.0627, Val Loss: 0.0148, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0212, Val Loss: 0.0140, Val F1 score 0.0961\n",
      "200\n",
      "1\n",
      "Epoch: 0, LR: 0.001, Train Loss: 0.0500, Val Loss: 0.0139, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0089, Val Loss: 0.0099, Val F1 score 0.3981\n",
      "Epoch: 20, LR: 0.0001, Train Loss: 0.0064, Val Loss: 0.0097, Val F1 score 0.4389\n",
      "Epoch: 30, LR: 1.0000000000000002e-06, Train Loss: 0.0061, Val Loss: 0.0097, Val F1 score 0.4419\n",
      "200\n",
      "2\n",
      "Epoch: 0, LR: 0.001, Train Loss: 0.0456, Val Loss: 0.0148, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0066, Val Loss: 0.0075, Val F1 score 0.5467\n",
      "Epoch: 20, LR: 0.001, Train Loss: 0.0016, Val Loss: 0.0064, Val F1 score 0.6978\n",
      "Epoch: 30, LR: 0.0001, Train Loss: 0.0005, Val Loss: 0.0067, Val F1 score 0.7269\n",
      "200\n",
      "3\n",
      "Epoch: 0, LR: 0.001, Train Loss: 0.0422, Val Loss: 0.0149, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0172, Val Loss: 0.0118, Val F1 score 0.1341\n",
      "Epoch: 20, LR: 0.001, Train Loss: 0.0066, Val Loss: 0.0075, Val F1 score 0.5925\n",
      "Epoch: 30, LR: 0.001, Train Loss: 0.0021, Val Loss: 0.0062, Val F1 score 0.7016\n",
      "Epoch: 40, LR: 1e-05, Train Loss: 0.0014, Val Loss: 0.0064, Val F1 score 0.7370\n",
      "Epoch: 50, LR: 1.0000000000000002e-07, Train Loss: 0.0013, Val Loss: 0.0064, Val F1 score 0.7376\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for unit in [100, 200]:\n",
    "    for num_layer in [1, 2, 3]:\n",
    "        print(unit)\n",
    "        print(num_layer)\n",
    "        model = BILSTM(emb_dim=500, lstm_unit=unit, num_layers=num_layer)\n",
    "        best_model, best_val_loss, best_val_f1 = train(train_data, val_data, model)\n",
    "        models.append({\"lstm_unit\": unit,\n",
    "                       \"num_layers\": num_layer,\n",
    "                       \"trained_model\": best_model, \n",
    "                       \"best_val_f1\": best_val_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_val_f1</th>\n",
       "      <th>lstm_unit</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>trained_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.414629</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.740605</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.096096</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.441942</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.734384</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.737609</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_val_f1  lstm_unit  num_layers  \\\n",
       "0     0.414629        100           1   \n",
       "1     0.740605        100           2   \n",
       "2     0.096096        100           3   \n",
       "3     0.441942        200           1   \n",
       "4     0.734384        200           2   \n",
       "5     0.737609        200           3   \n",
       "\n",
       "                                       trained_model  \n",
       "0  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...  \n",
       "1  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...  \n",
       "2  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...  \n",
       "3  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...  \n",
       "4  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...  \n",
       "5  BILSTM(\\n  (embedding): Embedding(1687, 500)\\n...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, LR: 0.001, Train Loss: 0.2256, Val Loss: 0.1109, Val F1 score 0.0961\n",
      "Epoch: 10, LR: 0.001, Train Loss: 0.0357, Val Loss: 0.0577, Val F1 score 0.5142\n",
      "Epoch: 20, LR: 0.001, Train Loss: 0.0100, Val Loss: 0.0484, Val F1 score 0.6688\n",
      "Epoch: 30, LR: 0.001, Train Loss: 0.0037, Val Loss: 0.0471, Val F1 score 0.7245\n",
      "Epoch: 40, LR: 0.001, Train Loss: 0.0017, Val Loss: 0.0488, Val F1 score 0.7277\n",
      "Epoch: 50, LR: 0.0001, Train Loss: 0.0009, Val Loss: 0.0489, Val F1 score 0.7406\n"
     ]
    }
   ],
   "source": [
    "### LSTM with added length as additional feature\n",
    "model = BILSTM_WITH_LEN(emb_dim=300, lstm_unit=150, num_layers=2)\n",
    "best_model, best_val_loss, best_val_f1 = train(train_data, val_data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data, m):\n",
    "    \"average f1 score of individual entry\"\n",
    "    m.eval()\n",
    "    total_f1 = 0\n",
    "    num_samples = 0\n",
    "    for x, y in data:\n",
    "        pred = m(x)\n",
    "        total_f1 += f1(pred, y)\n",
    "        num_samples += y.shape[0]\n",
    "    return(total_f1/num_samples)\n",
    "\n",
    "    \n",
    "def f1(y_pred, y_true):\n",
    "    total_f1 = 0\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).int().cpu().numpy()  \n",
    "    batch_size, num_class = y_true.shape    \n",
    "    for sample_idx in range(batch_size):\n",
    "        true_idx = np.arange(num_class)[(y_true[sample_idx] == 1).astype(bool)]\n",
    "        pred_idx = np.arange(num_class)[(y_pred[sample_idx] == 1).astype(bool)]\n",
    "        # make sure at least to predict one\n",
    "        assert (y_true[sample_idx].sum() > 0)\n",
    "        if len(pred_idx) == 0:\n",
    "            pred_idx = [np.argmax(y_pred[sample_idx]).item()]\n",
    "\n",
    "        tp = len(np.intersect1d(true_idx, pred_idx))        \n",
    "        precision = tp/len(pred_idx)\n",
    "        recall = tp/len(true_idx)\n",
    "        if (precision + recall) == 0:\n",
    "            f1_score = 0\n",
    "        else:\n",
    "            f1_score = 2 * precision * recall/(precision + recall)\n",
    "        total_f1 += f1_score\n",
    "    return total_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lstm_unit': 100, 'num_layers': 1, 'trained_model': BILSTM(\n",
      "  (embedding): Embedding(1687, 500)\n",
      "  (lstm): LSTM(500, 100, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=46, bias=True)\n",
      "), 'best_val_f1': 0.41462891462891466}\n",
      "train f1:,  0.6214038983469036\n",
      "val f1:,  0.41062491062491063\n",
      "{'lstm_unit': 100, 'num_layers': 2, 'trained_model': BILSTM(\n",
      "  (embedding): Embedding(1687, 500)\n",
      "  (lstm): LSTM(500, 100, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=46, bias=True)\n",
      "), 'best_val_f1': 0.7406048906048905}\n",
      "train f1:,  0.9864298050826548\n",
      "val f1:,  0.7305948805948805\n",
      "{'lstm_unit': 100, 'num_layers': 3, 'trained_model': BILSTM(\n",
      "  (embedding): Embedding(1687, 500)\n",
      "  (lstm): LSTM(500, 100, num_layers=3, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=46, bias=True)\n",
      "), 'best_val_f1': 0.0960960960960961}\n",
      "train f1:,  0.09733530717986677\n",
      "val f1:,  0.0960960960960961\n",
      "{'lstm_unit': 200, 'num_layers': 1, 'trained_model': BILSTM(\n",
      "  (embedding): Embedding(1687, 500)\n",
      "  (lstm): LSTM(500, 200, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=400, out_features=46, bias=True)\n",
      "), 'best_val_f1': 0.4419419419419419}\n",
      "train f1:,  0.7174932149025411\n",
      "val f1:,  0.4419419419419419\n",
      "{'lstm_unit': 200, 'num_layers': 2, 'trained_model': BILSTM(\n",
      "  (embedding): Embedding(1687, 500)\n",
      "  (lstm): LSTM(500, 200, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=400, out_features=46, bias=True)\n",
      "), 'best_val_f1': 0.7343843843843845}\n",
      "train f1:,  0.9951147298297557\n",
      "val f1:,  0.7227727727727727\n",
      "{'lstm_unit': 200, 'num_layers': 3, 'trained_model': BILSTM(\n",
      "  (embedding): Embedding(1687, 500)\n",
      "  (lstm): LSTM(500, 200, num_layers=3, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=400, out_features=46, bias=True)\n",
      "), 'best_val_f1': 0.7376090376090375}\n",
      "train f1:,  0.9737725141870223\n",
      "val f1:,  0.7376090376090375\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    print(m)\n",
    "    print(\"train f1:, \", evaluate(train_data, m[\"trained_model\"]))\n",
    "    print(\"val f1:, \", evaluate(val_data, m[\"trained_model\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_LOOKUP = np.array(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def x2text(seq, onehot=False):\n",
    "    if onehot:\n",
    "        idx = np.argmax(seq.cpu(), axis=2)\n",
    "    else:\n",
    "        idx = seq.cpu().numpy()\n",
    "    text = []\n",
    "    for i in range(seq.shape[1]):\n",
    "        text.append(VOCAB_LOOKUP[idx[:, i]])\n",
    "    return text\n",
    "    \n",
    "\n",
    "def y2text(manyhot_label):\n",
    "    labels = np.array(np.load(\"./data/labels.npy\"))\n",
    "    all_labels = []    \n",
    "    for i in range(manyhot_label.shape[0]):\n",
    "        assert manyhot_label[i].sum() > 0\n",
    "        all_labels.append(labels[manyhot_label[i].astype(bool)])\n",
    "    return(all_labels)\n",
    "\n",
    "\n",
    "def error_analysis(data, m, onehot=False):\n",
    "    for x, y in data:\n",
    "        pred = m(x)\n",
    "        batch_size = x.shape[1]\n",
    "        y_pred = (torch.sigmoid(pred) > 0.5).int().cpu().numpy()\n",
    "        for i in range(batch_size):\n",
    "            if y_pred[i, :].sum() == 0:\n",
    "                y_pred[i, np.argmax(y_pred[i, :])] = 1\n",
    "        \n",
    "        correct = np.all(y_pred==y, axis=1)\n",
    "        print(\"total correct: {} out of {}\".format(correct.sum(), batch_size))\n",
    "        \n",
    "        incorrect_idx = np.arange(batch_size)[(1-correct).astype(bool)]\n",
    "\n",
    "        x_text = x2text(x, onehot=onehot)\n",
    "        y_label_true = y2text(y)\n",
    "        y_label_pred = y2text(y_pred)\n",
    "        \n",
    "        for i in incorrect_idx:\n",
    "#             break\n",
    "            print(x_text[i])\n",
    "            print(\"--------------\")\n",
    "            print(\"true\", y_label_true[i])\n",
    "            print(\"pred\", y_label_pred[i])\n",
    "            print(\"correct?\", correct[i])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_analysis(val_data, models[1][\"trained_model\"], onehot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate submission file for kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_submission(m):\n",
    "    m.eval()\n",
    "    labels = np.load(\"./data/labels.npy\")\n",
    "    final_labels = []\n",
    "    tokenize = lambda x: x.split()\n",
    "    text_field = Field(sequential=True, \n",
    "                       tokenize=tokenize, \n",
    "                       lower=True, \n",
    "                       include_lengths=False)\n",
    "    tst_datafields = [(\"ID\", RawField()),\n",
    "                      (\"UTTERANCE\", TEXT)]\n",
    "    tst = TabularDataset(\n",
    "        path=\"data/original_data/hw1_test.csv\", # the file path\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tst_datafields)\n",
    "    test_iter = Iterator(tst, batch_size=len(tst), device='cuda', shuffle=False,\n",
    "                         sort=False, sort_within_batch=False, repeat=False)\n",
    "    for batch in test_iter:\n",
    "        pred = (torch.sigmoid(m(batch.UTTERANCE)) > 0.5).int().cpu()\n",
    "        for sample in pred:\n",
    "            if sample.sum() == 0:\n",
    "                pred_label = [labels[np.argmax(sample)]]\n",
    "            else:\n",
    "                pred_label = np.array(labels)[sample==1]\n",
    "            final_labels.append(\" \".join(pred_label))\n",
    "    test_df = pd.read_csv(\"./data/original_data/hw1_test.csv\")\n",
    "    test_df[\"CORE RELATIONS\"] = final_labels\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = get_submission(models[0][\"trained_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.today().strftime(\"%b%d\")\n",
    "test_df.set_index(\"ID\")[[\"CORE RELATIONS\"]].to_csv(\n",
    "    \"./data/submissions/{}.csv\".format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
