{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import importlib\n",
    "\n",
    "import data_utils\n",
    "import model_utils\n",
    "import train_utils\n",
    "import evaluation\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(model_utils)\n",
    "importlib.reload(train_utils)\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data and put in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(np.load(\"./data/vocab.npy\"))\n",
    "data_path = \"data\"\n",
    "train_file = \"train_real.csv\"\n",
    "val_file = \"val.csv\"\n",
    "test_file = \"holdout_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, val_data = data_utils.prep_train_val(\n",
    "    data_path, train_file, val_file, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model_utils.MultiLayerMLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, LR: 0.001, Train Loss: 14.4529, Val Loss: 14.9266, Val f1 0.096\n",
      "Epoch: 10, LR: 0.001, Train Loss: 3.7388, Val Loss: 3.7423, Val f1 0.503\n",
      "Epoch: 20, LR: 0.001, Train Loss: 2.1122, Val Loss: 2.3308, Val f1 0.710\n",
      "Epoch: 30, LR: 0.001, Train Loss: 1.4026, Val Loss: 2.0568, Val f1 0.776\n",
      "Epoch: 40, LR: 0.001, Train Loss: 1.0980, Val Loss: 2.1321, Val f1 0.782\n",
      "Epoch: 50, LR: 0.0001, Train Loss: 0.6411, Val Loss: 2.1379, Val f1 0.801\n",
      "Epoch: 60, LR: 1e-05, Train Loss: 0.5960, Val Loss: 2.1204, Val f1 0.803\n"
     ]
    }
   ],
   "source": [
    "result = train_utils.train(train_data, val_data, m, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter searching - MLP\n",
    "best model so far: 3 middle layer, 100 hidden units, dropout=0.2 at second to last layer\n",
    "\n",
    "saved as ./data/model_checkpoints/MLP_Jan23.mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Searching Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_LOOKUP = np.array(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def x2text(seq, onehot=False):\n",
    "    if onehot:\n",
    "        idx = np.argmax(seq.cpu(), axis=2)\n",
    "    else:\n",
    "        idx = seq.cpu().numpy()\n",
    "    text = []\n",
    "    for i in range(seq.shape[1]):\n",
    "        text.append(VOCAB_LOOKUP[idx[:, i]])\n",
    "    return text\n",
    "    \n",
    "\n",
    "def y2text(manyhot_label):\n",
    "    labels = np.array(np.load(\"./data/labels.npy\"))\n",
    "    all_labels = []    \n",
    "    for i in range(manyhot_label.shape[0]):\n",
    "        assert manyhot_label[i].sum() > 0\n",
    "        all_labels.append(labels[manyhot_label[i].astype(bool)])\n",
    "    return(all_labels)\n",
    "\n",
    "\n",
    "def error_analysis(data, m, onehot=False):\n",
    "    for x, y in data:\n",
    "        pred = m(x)\n",
    "        batch_size = x.shape[1]\n",
    "        y_pred = (torch.sigmoid(pred) > 0.5).int().cpu().numpy()\n",
    "        for i in range(batch_size):\n",
    "            if y_pred[i, :].sum() == 0:\n",
    "                y_pred[i, np.argmax(y_pred[i, :])] = 1\n",
    "        \n",
    "        correct = np.all(y_pred==y, axis=1)\n",
    "        print(\"total correct: {} out of {}\".format(correct.sum(), batch_size))\n",
    "        \n",
    "        incorrect_idx = np.arange(batch_size)[(1-correct).astype(bool)]\n",
    "\n",
    "        x_text = x2text(x, onehot=onehot)\n",
    "        y_label_true = y2text(y)\n",
    "        y_label_pred = y2text(y_pred)\n",
    "        \n",
    "        for i in incorrect_idx:\n",
    "#             break\n",
    "            print(x_text[i])\n",
    "            print(\"--------------\")\n",
    "            print(\"true\", y_label_true[i])\n",
    "            print(\"pred\", y_label_pred[i])\n",
    "            print(\"correct?\", correct[i])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_analysis(val_data, models[1][\"trained_model\"], onehot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate submission file for kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_submission(m):\n",
    "    m.eval()\n",
    "    labels = np.load(\"./data/labels.npy\")\n",
    "    final_labels = []\n",
    "    tokenize = lambda x: x.split()\n",
    "    text_field = Field(sequential=True, \n",
    "                       tokenize=tokenize, \n",
    "                       lower=True, \n",
    "                       include_lengths=False)\n",
    "    tst_datafields = [(\"ID\", RawField()),\n",
    "                      (\"UTTERANCE\", TEXT)]\n",
    "    tst = TabularDataset(\n",
    "        path=\"data/original_data/hw1_test.csv\", # the file path\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tst_datafields)\n",
    "    test_iter = Iterator(tst, batch_size=len(tst), device='cuda', shuffle=False,\n",
    "                         sort=False, sort_within_batch=False, repeat=False)\n",
    "    for batch in test_iter:\n",
    "        pred = (torch.sigmoid(m(batch.UTTERANCE)) > 0.5).int().cpu()\n",
    "        for sample in pred:\n",
    "            if sample.sum() == 0:\n",
    "                pred_label = [labels[np.argmax(sample)]]\n",
    "            else:\n",
    "                pred_label = np.array(labels)[sample==1]\n",
    "            final_labels.append(\" \".join(pred_label))\n",
    "    test_df = pd.read_csv(\"./data/original_data/hw1_test.csv\")\n",
    "    test_df[\"CORE RELATIONS\"] = final_labels\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = get_submission(models[0][\"trained_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.today().strftime(\"%b%d\")\n",
    "test_df.set_index(\"ID\")[[\"CORE RELATIONS\"]].to_csv(\n",
    "    \"./data/submissions/{}.csv\".format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
