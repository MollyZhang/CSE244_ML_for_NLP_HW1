{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import itertools\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset with movie and people names condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/train.csv\")\n",
    "df2 = pd.read_csv(\"./data/test.csv\")\n",
    "df = pd.concat((df1, df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_names = list(pd.read_csv(\"./data/movie_name_condensed_data/movie_names.csv\").as_matrix().flatten())\n",
    "movie_names.append(\"apollo thirteen\")\n",
    "movie_names.append(\"the amazing spider man\")\n",
    "movie_names.extend([\"the hobbist\", \"rocky 2\", \"i love lucy\", \"appolo thirteen\", \"pretty girl\"])\n",
    "movie_names.extend([\"modern family\", \"pretty women\", \"passion of the christ\", \"the hulk\", \n",
    "                    \"lion king\", \"les miserable\", \"scooby do\", \"amazing spiderman\", \"forest gump\", \"e t\",\n",
    "                    \"shindler's list\", \"life is beatiful\", \"downton abbey\", \"edward scissor hands\", \n",
    "                    \"mr and mrs smith\", \"marley and me\", \"harry potter\",\"dark knight\",\"a mom for christmas\",\n",
    "                    \"caddy shack\", \"star wars new hope\", 'the house at the end of the street', \"the king is back\", \n",
    "                    \"she s the man\", \"lord of the rings\", \"the land of blood and honey\", \"the santa claus\", \n",
    "                    \"the father of my children\", \"lord of the flys\", \"wizard of oz\", \"one life to live\", \n",
    "                    \"i am legend 2\", \"toy story 4\"\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_names = [\"homer simpson\", \"kristen stewart\", \"steven spielberg\", \"richard lester\", \"robert wise\"\n",
    "                \"woody allen\", \"charles vidor\", \"ray stark\", \"will smith\", \"kevin james\", \"tom hanks\", \n",
    "                \"tom cruise\", \"victor fleming\", \"angelina jolie\", \"albert ruddy\", \"alfred hitchcock\", \n",
    "                \"ed harris\", \"bette midler\", \"sandra bullock\", \"bruce lee\", \"charles vidor\", \"noah baumbach\",\n",
    "                \"todd solondz\", \"brad pitt\", \"frances ford copolla\", \"clint eastwood\", \"karan johar\", \"will ferrell\",\n",
    "                \"james brown\", \"larry clark\", \"jennifer aniston\", \"robert wise\", \"james cameron\", \"david selsnic\", \n",
    "                \"arthur rudy\", 'alfred hitchock', \"patrick swayze\",\"dustin hoffman\", \"julia roberts\", \"ridley scott\",\n",
    "                \"miranda july\",\"oliver stone\", \"yash chopra\", 'penny marshall', \"kevin spacey\", 'hugh jackman', \n",
    "                \"quentin tarantino\", \"gwyneth paltrow\", \"spike lee\", \"robert redford\", \"george lucas\",\"jj abrams\",\n",
    "                \"robert deniro\", \"chris columbus\", \"martin scorcese\", \"tony scott\", \"niel abramson\", \"roger rabbit\", \n",
    "                \"the zucker brothers\", \"von sudow\", \"lee unkridge\",\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "producer_names = [\"searchlight films\", \n",
    "                  \"warner bros.\", \"7 arts\", \n",
    "                  \"castle rock entertainment\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actoress_names = list(pd.read_csv(\n",
    "    \"./data/movie_name_condensed_data/actoress_names.csv\", header=None).as_matrix().flatten())\n",
    "actoress_names = [i for i in actoress_names if len(i.split(\" \"))>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000,110000,120000,130000,140000,150000,160000,170000,180000,190000,200000,210000,220000,230000,"
     ]
    },
    {
     "data": {
      "text/plain": [
       "665"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_names = movie_names + person_names + actoress_names + producer_names\n",
    "# filter names that show up in data\n",
    "all_text = \" \".join(df.text)\n",
    "filtered_names = []\n",
    "for i, name in enumerate(all_names):\n",
    "    if i % 10000 == 0:\n",
    "        print(i, end=\",\")\n",
    "    if name in all_text:\n",
    "        filtered_names.append(name)\n",
    "len(filtered_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_names(text, names):\n",
    "    for name in names:\n",
    "        if name in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "NAME_MAPPER = {\"apollo thirteen\": \"apollo 13\",\n",
    "               \"apollo thirteen\": \"apollo 13\", \n",
    "               \"appolo thirteen\": \"apollo 13\", \n",
    "               \"steven spielberg\": \"spielberg\",\n",
    "               \"stephen spielberg\": \"spielberg\", \n",
    "               \"spider man\": \"spiderman\",\n",
    "               \"childs vidor\": \"charles vidor\",\n",
    "               \" e t\": \" et\",\n",
    "               \"alfred hitchock\": \"hitchcock\",\n",
    "               \"alfred hitchcock\": \"hitchcock\", \n",
    "               \"warner brothers\": \"warner bros.\",\n",
    "               \"albert rudy\": \"albert ruddy\",\n",
    "               \"will ferell\": \"will ferrell\",\n",
    "               \"the god father\": \"the godfather\"}\n",
    "\n",
    "NAMES = filtered_names + [i for i in NAME_MAPPER.values() \n",
    "                          if len(i.split(\" \"))>1 and i!=\" et\"]\n",
    "remove = [\"show me\", \"a movie\", \"the movies\", \"the company\", \"tell me the\", \"me the\", \n",
    "          \"are the\", \"tell me\", \"e t\", \"i am\", \"the actors\", \"ng ho\", \"ed ma\", \"al lang\", \"d day\", \n",
    "          \"in view\", \"the pass\", \"the house\", \"the life\", \"the box\", \"take me\", \"the star\", \n",
    "          \"the passion\", \"the king\", \"the man\", \"the giant\", \"the end\", \"the ring\", \"love stories\", \n",
    "          \"the land\", \"exhibit a\", \"de palma\", \"blind side\", \"santa claus\", \"the sixties\", \"ma ma\", \n",
    "          \"in orange\", \"the sin\", \"parental guidance\", \"the sand\", \"the boy\", \"about love\", \"all in\", \n",
    "          \"the kids\", \"the green\", \"the car\", \"the mother\", \"the father\", \"the voices\", \"the fly\", \n",
    "          \"made in france\", \"bad guy\", \"to live\", \"beautiful people\", \"top five\", \"i do\", \"i come\", \n",
    "          \"about time\", \"in bar\", \"tin man\", 'new york', \"my boy\", \"one life\", \"time please\", \n",
    "          \"love life\", \"our time\", \"i am i\", \"i want you\", \"the last movie\", \"the first star\", \n",
    "          \"you get me\"] + list(NAME_MAPPER.keys())\n",
    "the_words = [i for i in NAMES if len(i.split(\" \"))==2 and i.startswith(\"the\")]\n",
    "NAMES = [i for i in NAMES if i not in remove and i not in the_words]\n",
    "NAMES = sorted(list(set(NAMES)), key=lambda x:len(x.split(\" \")), reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "def map_names(text):\n",
    "    for name, value in NAME_MAPPER.items():\n",
    "        text = text.replace(name, value)\n",
    "    return text\n",
    "\n",
    "def phrasify(text):\n",
    "    for name in NAMES:\n",
    "        if name in text:\n",
    "            phrase_name = name.replace(\" \", \"|\").replace(\"'\", \"|\")\n",
    "            text = text.replace(name, phrase_name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/train.csv\", index_col=\"ID\")\n",
    "df2 = pd.read_csv(\"./data/test.csv\", index_col=\"ID\")\n",
    "df = pd.concat((df1, df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"text\"] = df1[\"text\"].apply(map_names).apply(phrasify)\n",
    "df2[\"text\"] = df2[\"text\"].apply(map_names).apply(phrasify)\n",
    "df1[\"raw_text\"] = df1[\"text\"]\n",
    "df2[\"raw_text\"] = df2[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.to_csv(\"./data/movie_name_condensed_data/train.csv\")\n",
    "df2.to_csv(\"./data/movie_name_condensed_data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "df = pd.read_csv(\"./data/movie_name_condensed_data/train.csv\", \n",
    "                 index_col=\"ID\")\n",
    "split_size = int(df.shape[0]/10)\n",
    "np.random.seed(0)\n",
    "\n",
    "test_idx = np.random.choice(df.index, size=split_size)\n",
    "rest = np.array([i for i in df.index if i not in test_idx])\n",
    "val_idx = np.random.choice(rest, size=split_size)\n",
    "train_idx = np.array([i for i in rest if i not in val_idx])\n",
    "\n",
    "df.loc[train_idx].to_csv(\"./data/movie_name_condensed_data/train_real.csv\")\n",
    "df.loc[rest].to_csv(\"./data/movie_name_condensed_data/train_val.csv\")\n",
    "df.loc[val_idx].to_csv(\"./data/movie_name_condensed_data/val.csv\")\n",
    "df.loc[test_idx].to_csv(\"./data/movie_name_condensed_data/holdout_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vocab 1664\n",
      "test vocab 957\n",
      "Combined vocab 1910\n",
      "test vocab not in train 246\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "import re\n",
    "\n",
    "df1 = pd.read_csv(\"./data/movie_name_condensed_data/train.csv\")\n",
    "df2 = pd.read_csv(\"./data/movie_name_condensed_data/test.csv\")\n",
    "\n",
    "splitter = lambda x: re.split(\" |'\", x.lower())\n",
    "\n",
    "vocab1 = set(list(itertools.chain.from_iterable(list(df1[\"text\"].apply(splitter)))))\n",
    "vocab2 = set(list(itertools.chain.from_iterable(list(df2[\"text\"].apply(splitter)))))\n",
    "vocab = sorted(list(vocab1.union(vocab2)))\n",
    "print(\"train vocab\", len(vocab1))\n",
    "print(\"test vocab\", len(vocab2))\n",
    "print(\"Combined vocab\", len(vocab))\n",
    "print(\"test vocab not in train\", len([i for i in vocab2 if i not in vocab1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./data/movie_name_condensed_data/vocab.npy\", np.array(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000,1100000,1200000,1300000,1400000,1500000,1600000,1700000,1800000,1900000,2000000,"
     ]
    }
   ],
   "source": [
    "f = open(\"../fasttext/crawl-300d-2M-subword.vec\", \"r\", encoding=\"utf-8\")\n",
    "f_out = open(\"../fasttext/hw1_vocab_phrase.vec\", \"w\")\n",
    "f_out.write(\"{} {}\\n\".format(len(vocab), 300))\n",
    "d = {}\n",
    "for i, line in enumerate(f):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if i % 100000 == 0:\n",
    "        print(i, end=\",\")\n",
    "    word = line.split(\" \")[0]\n",
    "    if word in vocab:\n",
    "        d[word] = line.strip().split(' ')\n",
    "        f_out.write(line)\n",
    "    if len(d) == len(vocab):\n",
    "        break\n",
    "f.close()\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(d, open(\"./data/movie_name_condensed_data/vocab_ft.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "\n",
    "train_val_data, holdout_test_data, test_data = data_utils.prep_all_data(\n",
    "    path=\"./data/movie_name_condensed_data/\",\n",
    "    train_file=\"train_val.csv\",\n",
    "    val_file=\"holdout_test.csv\",\n",
    "    test_file=\"test.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = train_val_data.text_field.vocab.itos\n",
    "ft_emb = pickle.load(open(\"./data/movie_name_condensed_data/vocab_ft.pkl\", \"rb\"))\n",
    "emb_matrix = []\n",
    "num_unknown = 0\n",
    "for word in vocab:\n",
    "    if word in ft_emb:\n",
    "        vec = ft_emb[word][1:]\n",
    "        emb_matrix.append([float(i) for i in vec])\n",
    "    else:\n",
    "        num_unknown += 1\n",
    "        vec = [np.random.normal() for i in range(300)]\n",
    "        emb_matrix.append(vec)\n",
    "emb_matrix = torch.tensor(emb_matrix)\n",
    "torch.save(emb_matrix, \"./data/movie_name_condensed_data/emb_matrix_ft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3338, 4)\n",
      "(1084, 4)\n",
      "(4422, 4)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"./data/movie_name_condensed_data/train.csv\", index_col=\"ID\")\n",
    "df2 = pd.read_csv(\"./data/movie_name_condensed_data/test.csv\", index_col=\"ID\")\n",
    "df = pd.concat([df1, df2])\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1910"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.load(\"./data/movie_name_condensed_data/vocab.npy\"); len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ngram(sent, gram):\n",
    "    words = re.split(\"'| \", sent.lower())\n",
    "    if len(words) < gram:\n",
    "        return []\n",
    "    ngrams = []\n",
    "    for i in range(len(words)-(gram-1)):\n",
    "        ngrams.append(\"_\".join(words[i:i+gram]))\n",
    "    return ngrams\n",
    "\n",
    "def save_ngram(df, n=2):\n",
    "    all_ngrams = []\n",
    "    for ngrams in df.raw_text.apply(lambda x: get_ngram(x, n)):\n",
    "        all_ngrams.extend(ngrams)\n",
    "    all_ngrams = np.array(sorted(list(set(all_ngrams))))\n",
    "    np.save(\"./data/movie_name_condensed_data/{}grams.npy\".format(n), all_ngrams)\n",
    "    print(\"number of {} grams: {}\".format(n, len(all_ngrams)))\n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 1 grams: 1910\n",
      "number of 2 grams: 6660\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 3):\n",
    "    save_ngram(df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import importlib\n",
    "import pickle\n",
    "\n",
    "import data_utils\n",
    "import model_utils\n",
    "import train_utils\n",
    "import evaluation\n",
    "import submission\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(model_utils)\n",
    "importlib.reload(train_utils)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(submission)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"./data/movie_name_condensed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = data_utils.prep_all_data(path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, LR: 0.01, Train Loss: 233.9552, Val Loss: 99.2596, Val f1 0.755\n",
      "Epoch: 5, LR: 0.01, Train Loss: 3.4232, Val Loss: 69.5909, Val f1 0.828\n",
      "Epoch: 10, LR: 0.01, Train Loss: 1.0634, Val Loss: 82.7617, Val f1 0.837\n",
      "Epoch: 15, LR: 0.001, Train Loss: 0.4305, Val Loss: 87.2950, Val f1 0.844\n",
      "Epoch: 20, LR: 0.001, Train Loss: 0.3631, Val Loss: 86.7176, Val f1 0.843\n",
      "Epoch: 25, LR: 0.0001, Train Loss: 0.3160, Val Loss: 87.8480, Val f1 0.840\n",
      "Epoch: 30, LR: 0.0001, Train Loss: 0.2400, Val Loss: 88.2805, Val f1 0.838\n",
      "Epoch: 35, LR: 1e-05, Train Loss: 0.2772, Val Loss: 88.1830, Val f1 0.842\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(model_utils)\n",
    "importlib.reload(data_utils)\n",
    "\n",
    "m = model_utils.BaseModelNGram(path=PATH)\n",
    "result = train_utils.train(train_data, val_data, m,\n",
    "                          lr=1e-2, print_freq=5, max_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result[\"trained_model\"], \"./data/model_checkpoints/ngram_phrase_MLP_Jan31.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, LR: 0.01, Train Loss: 318.1742, Val Loss: 313.7753, Val f1 0.404\n",
      "Epoch: 5, LR: 0.01, Train Loss: 17.0051, Val Loss: 108.0352, Val f1 0.774\n",
      "Epoch: 10, LR: 0.01, Train Loss: 2.8036, Val Loss: 113.0202, Val f1 0.791\n",
      "Epoch: 15, LR: 0.01, Train Loss: 1.0752, Val Loss: 120.3200, Val f1 0.791\n",
      "Epoch: 20, LR: 0.001, Train Loss: 0.7764, Val Loss: 117.6829, Val f1 0.790\n",
      "Epoch: 25, LR: 0.001, Train Loss: 0.8133, Val Loss: 118.0313, Val f1 0.795\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(model_utils)\n",
    "importlib.reload(data_utils)\n",
    "\n",
    "m = model_utils.GRU(path=PATH)\n",
    "result = train_utils.train(train_data, val_data, m,\n",
    "                          lr=1e-2, print_freq=5, max_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(result[\"trained_model\"], \"./data/model_checkpoints/GRU_phrase_Jan31.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_phrase, val_data_phrase, test_data_phrase = data_utils.prep_all_data(path=PATH)\n",
    "train_data, val_data, test_data = data_utils.prep_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/anaconda3/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'model_utils.BaseModelNGram' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/molly/anaconda3/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'model_utils.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble f1 val score: 0.8839173405211147\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submission)\n",
    "\n",
    "m_gram = torch.load(\"./data/model_checkpoints/ngram_MLP_Jan30.mdl\")\n",
    "m_gram_phrase = torch.load(\"./data/model_checkpoints/ngram_phrase_MLP_Jan31.mdl\")\n",
    "m_GRU = torch.load(\"./data/model_checkpoints/GRU_Jan30.mdl\")\n",
    "m_GRU_phrase = torch.load(\"./data/model_checkpoints/GRU_phrase_Jan31.mdl\")\n",
    "ensemble = submission.Ensemble(model1=m_gram, model2=m_gram_phrase, \n",
    "                               m1_val_data=val_data, m1_test_data=test_data,\n",
    "                               m2_val_data=val_data_phrase, m2_test_data=test_data_phrase)\n",
    "df = ensemble.get_ensemble_result(submission=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble f1 val score: 0.8955974842767297\n"
     ]
    }
   ],
   "source": [
    "ensemble = submission.Ensemble(model1=m_GRU_phrase, model2=m_gram_phrase, \n",
    "                               m1_val_data=val_data_phrase, m1_test_data=test_data_phrase,\n",
    "                               m2_val_data=val_data_phrase, m2_test_data=test_data_phrase)\n",
    "df = ensemble.get_ensemble_result(submission=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble f1 val score: 0.8553459119496857\n"
     ]
    }
   ],
   "source": [
    "ensemble = submission.Ensemble(model1=m_GRU_phrase, model2=m_GRU, \n",
    "                               m1_val_data=val_data_phrase, m1_test_data=test_data_phrase,\n",
    "                               m2_val_data=val_data, m2_test_data=test_data)\n",
    "df = ensemble.get_ensemble_result(submission=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8476476476476476"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.calculate_f1(val_data_phrase, m_gram_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375375375375375"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.calculate_f1(val_data, m_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
